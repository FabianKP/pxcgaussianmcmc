
\documentclass[10pt]{article}

%~~~~~~~~~~
% preamble
%~~~~~~~~~~

%'
%' common packages
%'
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{amsmath,amssymb,dsfont}
\numberwithin{equation}{section}
\usepackage{microtype}
\usepackage{graphicx,tikz,pgfplots}
\graphicspath{{images/}}
\pgfplotsset{compat=newest}
\usepackage[hyperref,amsmath,thmmarks]{ntheorem}
\usepackage{aliascnt}
\usepackage[a4paper,centering,bindingoffset=0cm,marginpar=2cm,margin=2.5cm]{geometry}
%\usepackage[pagestyles]{titlesec}
\usepackage[font=footnotesize,format=plain,labelfont=sc,textfont=sl,width=0.75\textwidth,labelsep=period]{caption}
\usepackage{bm}
\usepackage{bbm}
\usepackage{algorithm, algpseudocode}

%'
%' biblatex
%'
\usepackage[backend=biber,maxnames=10,backref=true,hyperref=true,giveninits=true,safeinputenc]{biblatex}
\bibliography{proximal_mcmc.bib}


\DefineBibliographyStrings{english}{%
	backrefpage = {cited on page},
	backrefpages = {cited on pages},
}


\title{Proximal MCMC for linearly constrained multivariate normal distributions}
\author{FabianKP}
\date{}

%'
%' writes the title always in quotes.
%'
\DeclareFieldFormat[report]{title}{``#1''}
\DeclareFieldFormat[book]{title}{``#1''}
\AtEveryBibitem{\clearfield{url}}
\AtEveryBibitem{\clearfield{note}}


\usepackage[pdftex,colorlinks=true,linkcolor=blue,citecolor=green,urlcolor=blue,bookmarks=true,bookmarksnumbered=true]{hyperref}
\hypersetup
{
    pdfauthor={FabianKP},
    pdfsubject={Proximal MCMC for linearly constrained multivariate normal distributions},
    pdftitle={}
}
\def\sectionautorefname{Section}
\def\subsectionautorefname{Section}
\def\subsubsectionautorefname{Section}

\graphicspath{{images/}}

%
% --------------- ABBREVIATIONS
%

\include{abbreviations}


%
% --------------- BEGIN DOCUMENT
%


\begin{document}

\maketitle

\section*{Notation}

\begin{itemize}
\item $\indicator{C}$ denotes the indicator function,
\begin{align*}
\indicator{C}(x) := \begin{cases}
1, & \text{if } x \in C, \\
0, & \text{otherwise}.
\end{cases}
\end{align*}
\item $\chi_{C}$ denotes the characteristic function,
\begin{align*}
\chi_C(x) := \begin{cases}
0, & \text{if } x \in C, \\
\infty, & \text{otherwise}.
\end{cases}
\end{align*}
(Not to be confused with $\chi_{q, n}^2$, which denotes the $q$-quantile of the chi-squared distribution with $n$ degrees of freedom.)
\end{itemize}


\section{Introduction}

We consider linearly constrained multivariate normal distributions on $\R^d$. Such distributions have the general form

\begin{equation}
\begin{aligned}
p(\bm x) & \propto \exp \left( - \frac{1}{2} (\bm x - \bm m)^\top \bm \Sigma^{-1} (\bm x - \bm m) \right) \indicator{\mathcal C}(\bm x), \\
\mathcal C &= \Set{\bm x \in \R^d}{\bm A \bm x = \bm b, \bm C \bm x \geq \bm d, \bm l \leq \bm x \leq \bm u}.
\end{aligned}\label{eq:pdf_basic_form}
\end{equation}

Sampling from this distribution is important in a range of applications, in particular in Bayesian inverse problems with constraints. In such applications, the dimension $d$ will often be very large, and the covariance matrix $\bm \Sigma$ will sometimes be ill-conditioned. In that case, sampling from $p$ using non-specialized methods does not work.

One method that was proposed for sampling from ill-conditioned log-concave distributions such as these is the so-called \textbf{proximal Markov chain Monte Carlo method} (proximal MCMC) \cite{Per16, DurMouPer18}.

\subsection{Bringing your system in the right form}

If your denstiy comes from regression or Bayesian parameter estimation, it might not directly be available in the above form. In such situations, the density will be of the form
\begin{align}
p(\*x) \propto \exp \left( - \frac{1}{2} (L x - y)^\top R^{-1} (Lx - y) - \frac{1}{2} (x - z)^\top P^{-1} (x - z) \right) \indicator{\mathcal C}(\bm x), \label{eq:pdf_likelihood_form}
\end{align}
where $L: \R^d \to \R^m$ is some sort of observation matrix, $P \in \R^{d \times d}$ and $R \in \R^{m \times m}$ are the covariance matrices of the prior and likelihood, and $\indicator{\mathcal C}$ is as above. This distribution can be brought in the form \eqref{eq:pdf_basic_form} using the following identities:
\begin{align}
\* \Sigma^{-1} &= P^{-1} + L^\top R^{-1} L, \label{eq:sigma_formula} \\
\* m &= z + \Sigma L^\top R^{-1} (y - L z). \label{eq:m_formula}
\end{align}
If you don't want the prior term in \eqref{eq:pdf_likelihood_form} (i.e. the term with $P^{-1}$), the corresponding formulas are
\begin{align*}
\* \Sigma^{-1} & = (L^\top R^{-1} L)^{-1}, \\
\* m & = \Sigma L^\top R^{-1} y.
\end{align*}




\section{Description of proximal MCMC}

\subsection{Basic terminology and definitions}

In the following, let $p: \R^d \to [0, \infty)$ be a probability density function of the form
\begin{align*}
p(\bm x) = \frac{1}{Z} e^{-f(\bm x)},
\end{align*}
where $Z > 0$ is a constant and $f: \R^d \to (-\infty, \infty]$ is a convex function.

Given $\lambda > 0$, we define the \emph{proximal operator} $\prox{f}{\lambda}: \R^d \to \R$ associated to $f$ and $\lambda$ as
\begin{align*}
\prox{f}{\lambda}(\bm x) = \argmin_{\bm z} \lbrace f(z) + \frac{1}{2 \lambda} \norm{z - x}_2^2 \rbrace.
\end{align*}
Furthermore, we define the \emph{Moreau-Yosida envelope} $f_\lambda$ associated to $f$ and $\lambda$ as
\begin{align*}
f_\lambda(\bm x) = \inf_{\bm z}  \lbrace f(\bm z) + \frac{1}{2 \lambda} \norm{z - x}_2^2 \rbrace.
\end{align*}

\subsection{High-level description}

\subsubsection{Px-MALA}

The proximal Metropolis-adjusted Langevin algorithm (Px-MALA) is a special case of the general Metropolis-Hastings scheme. This means it creates a Markov chain $(\bm x_1, \ldots, \bm x_N)$ by repeating the following two steps for $n=0,\ldots, N$ (the first sample $x_0$ is given):
\begin{enumerate}
\item Given the current iterate $\bm x_n \in \R^d$, create a proposal $\bm y_{n+1} \sim q(\cdot | \bm x_n)$ from the proposal kernel $q(\cdot | \cdot)$.
\item Compute the Hastings-ratio
\begin{align*}
r_{n+1} = \min \left(1, \frac{p(\bm y_n) q(\bm x_n| \bm y_n)}{p(\bm x_n) q(\bm y_n| \bm x_n)} \right).
\end{align*}
Then, with probabilty $r$, set $\bm x_{n+1} = \bm y_n$. Otherwise, set $\bm x_{n+1} = \bm x_n$.
\end{enumerate}

Proximal MCMC uses the following proposal kernel:
\begin{align*}
q(\bm y | \bm x) =  \normal\left( \prox{f}{\delta}(\bm x), 2 \delta \Idmat_d \right).
\end{align*}
That is, given $\bm x \in \R^d$, we can generate a new proposal $\bm y$ by sampling $\bm z \sim \normal(0, \Idmat)$ and then setting
\begin{align*}
\bm y = \prox{f}{\delta}(\bm x) + \sqrt{2 \delta} \bm z.
\end{align*}

\subsubsection{MYMALA}

Alternatively, let
\begin{align*}
f(\bm x) = h(\bm x) + g(\bm x),
\end{align*}
where we assume that $h$ is twice continuously differentiable. Let $\lambda > 0$ and $(\delta_n)_{n=1}^\infty$ be a nonincreasing sequence of positive stepsizes. We define the so-called \emph{Moreau-Yosida Unadjusted Langevin Algorithm} (MYULA) by the iteration
\begin{align*}
& z_{n+1} \sim \normal(0, \Idmat), \\
& x_{n+1} = x_n - \delta_{n+1} \left[ \nabla h(x_n) + \lambda^{-1}( x_n - \prox{g}{\lambda}(x_n)) \right] + \sqrt{2 \delta_{n+1}} z_{n+1}. \\
\end{align*}
It is known that MYULA creates biased samples. Furthermore, due to the added noise there is no guarantee that the individual samples satisfy the constraints. As a remedy, the MYULA method can be embedded into a Metropolis-Hastings scheme. That is, we use the general scheme outlined above, but with the non-stationary proposal distribution
\begin{align}
q_n(\*y | \*x_n) = \normal(\*x_n - \delta_{n+1}\left[ \nabla h(\*x_n) + \lambda^{-1} ( \*x_n - \prox{g}{\lambda}(\*x_n) )\right], 2 \delta_{n+1} \Idmat_d). \label{eq:mymala_proposal}
\end{align}
The resulting algorithm is called MYMALA, for "Moreau-Yosida Metropolis-adjusted Langevin Algorithm".


An important caveat for the samples obtained from MYMALA is that they represent a smoothed distribution
\begin{align*}
p_\lambda(\bm x) = \frac{1}{Z} e^{-h(\bm x) + g_\lambda(\bm x)}.
\end{align*}
Hence, when estimating a quantity of interest
\begin{align*}
\bar \phi = \int \phi(\bm x) p(\bm x) \d x,
\end{align*}
instead of estimating it by an equally weighted sum over the samples,
\begin{align*}
\bar \phi^\text{bad} = \frac{1}{N} \sum_{n=1}^N \phi(x_n),
\end{align*}
one should use importance sampling,
\begin{align*}
\bar \phi^\text{good} = \sum_{n=1}^N w_n \phi(x_k),
\end{align*}
where
\begin{align*}
& w_n := \frac{\delta_n e^{\bar g_\lambda(\bm x_n)}}{\sum_{m=1}^N \delta_m e^{\bar g_\lambda(\bm x)}}, \\
& \bar g_\lambda(\bm x) = g_\lambda(\bm x) - g(\bm x).
\end{align*}

\subsection{Specialization to constrained Gaussians}

\subsubsection{PxMALA}

In our particular case, we have
\begin{align*}
f(\bm x) = \frac{1}{2}(\bm x - \bm m)^\top \bm \Sigma^{-1} (\bm x - \bm m) + \chi_{\mathcal C}(\bm x).
\end{align*}
In order to evaluate the proximal operator $\prox{f}{\lambda}$, one has to solve a constrained least-squares problem
\begin{align*}
\prox{f}{\lambda}(\bm x) = \argmin_{\bm z} \Set{\norm{\bm \Sigma^{-1/2}(\bm z - \bm m)}_2^2 + \frac{1}{\lambda} \norm{\bm z - \bm x}_2^2}{\bm A \bm z = \bm b, \bm C \bm z \geq \bm d, \bm l \leq \bm z \leq \bm u}.
\end{align*}
This problem can be solved fast using any method for quadratic optimization.

\subsubsection{MYMALA}

For MYMALA, we use the following splitting:
\begin{align*}
& f(\bm x) = h(\bm x) + g(\bm x), \\
& h(\bm x) = \frac{1}{2} (\bm x - \bm m)^\top \bm \Sigma^{-1} (\bm x - \bm m), \\
& g(\bm x) = \chi_{\mathcal C}(\bm x), \\
& \mathcal C = \Set{\bm x \in \R^d}{\bm A \bm x = \bm b, \bm C \bm x \geq \bm d, \bm l \leq \bm x \leq \bm u}.
\end{align*}
Performing a MYMALA-step thus requires evaluation of
\begin{align*}
\nabla h(\bm x) = \Sigma^{-1}(\bm x - \bm m)
\end{align*}
and
\begin{align*}
\prox{g}{\lambda}(\bm x) = \argmin_{\bm z} \Set{\norm{\bm z - \bm x}_2^2}{\bm A \bm x = \bm b, \bm C \bm x \geq \bm d, \bm l \leq \bm x \leq \bm u}.
\end{align*}
(since minimizing $\frac{1}{\lambda} \norm{\bm z - \bm x}$ is equivalent to minimizing $\norm{\bm z - \bm x}$). The latter can again be implemented using any general-purpose quadratic optimization solver.

\subsection{Choosing the hyper-parameters}

Finally, let us provide some guidelines on how to choose the hyperparameters for Px-MALA and MYMALA:

\subsubsection*{Px-MALA}

The stepsize parameter $\delta$ should usually be small. A common rule-of-thumb is that one should choose $\delta$ such that the acceptance frequency is around $0.5$. Our implementation provides an option to automatically adapt $\delta$ during burnin to achieve a prescribed acceptance frequency.

\subsubsection*{MYMALA}

Since $\lambda$ is a regularization parameter, we want to choose it small enough that the approximation error is not too large. On the other hand, we want to choose the stepsizes $\delta_n$ large in order to quickly explore the parameter spae. From the theory of MALA, we know that the sequence of stepsizes $(\delta_n)_{n=1}^\infty$ should be nonincreasing and satisfy $\delta_n < \frac{2}{\mathrm{Lip}(h) + 1/\lambda}$ for all $n \in \N$, where $\mathrm{Lip}(h)$ is the Lipschitz constant of $\nabla h$. Hence, one has to balance between the approximation error and the stepsizes.

Since in our case, $\nabla h = \*\Sigma^{-1}(\*x - \*m)$, we have $\mathrm{Lip}(h) = \norm{\*\Sigma^{-1}}_2$, and therefore, a default choice for $(\delta_n)$ might be the constant sequence
\begin{align*}
\delta_n = \frac{2}{\norm{\* \Sigma^{-1}}_2 + \frac{1}{\lambda}}.
\end{align*}

\subsection{Pseudocode}

\subsubsection*{PxMALA}

Note that the proposal kernel for Px-MALA satisfies
\begin{align}
\log q(\bm y | \bm x) = - \frac{1}{4\delta}\norm{\bm y - \prox{f}{\delta}(\bm x)}_2^2. \label{eq:pxmala_log_proposal}
\end{align}
Note that for the evaluation of the Hastings ratio, it is easier to work with log-probabilities. That is, instead of computing
\begin{align*}
r = \min \left( 1, \frac{p(\bm y) q(\bm x | \bm y)}{p(\bm x) q(\bm y | \bm x)} \right),
\end{align*}
it is better to compute (using \eqref{eq:pxmala_log_proposal}, and noting that the normalization constants cancel out)
\begin{align*}
s & := \log \left(\frac{p(\bm y) q(\bm x | \bm y)}{p(\bm x) q(\bm y | \bm x)} \right) \\
& = \log p(\bm y) - \log p(\bm x) + \log q(\bm x | \bm y) - \log q(\bm y | \bm x) \\
& = - h(\bm y) + h(\bm x) - \frac{1}{4 \delta}\norm{\bm x - \prox{f}{\delta}(\bm y)}_2^2 + \frac{1}{4 \delta}\norm{\bm y - \prox{f}{\delta}(\bm x)}_2^2
\end{align*}
and then to set $r = \min(1, e^s)$.

\begin{algorithm}[H]
\caption{\texttt{Px-MALA}}
Given a feasible point $\bm x_0 \in \R^d$, $\delta > 0$, and sample size $N \in \N$;
\begin{algorithmic}[2]\label{alg:pxmala}
\For{$n=0, 1,2,\ldots, N$}
\State $\bm x \gets \bm x_n$;
\State Solve
\begin{align*}
\min_{\bm \xi} & \quad \lbrace \norm{\bm \Sigma^{-1/2}(\bm \xi - \bm m)}_2^2 + \frac{1}{\delta} \norm{\bm \xi - \bm x}_2^2 \rbrace \\
\text{s. t.} & \quad \bm A \bm \xi = \bm b, \quad \bm C \bm \xi \geq \bm d, \quad \bm l \leq \bm \xi \leq \bm u,
\end{align*}
and store the minimizer as $\bm \xi$;
\State Sample $\bm z \sim \normal(0, \Idmat_d)$;
\State Set $\bm y \gets \bm \xi + \sqrt{2 \delta} \bm z$;
\If{$y$ violates a constraint}
\State $\bm x_{n+1} \gets \bm x$;
\Else
\State Solve
\begin{align*}
\min_{\bm \zeta} & \quad \lbrace \norm{\bm \Sigma^{-1/2}(\bm \zeta - \bm m)}_2^2 + \frac{1}{\delta} \norm{\bm \zeta - \bm y}_2^2 \rbrace \\
\text{s. t.} & \quad \bm A \bm \zeta = \bm b, \quad \bm C \bm \zeta \geq \bm d, \quad \bm l \leq \bm \zeta \leq \bm u,
\end{align*}
and store the minimizer as $\bm \zeta$;
\State $h \gets \frac{1}{2} \norm{\bm \Sigma^{-1/2}(\bm x - \bm m)}_2^2$;
\State $\tilde h = \frac{1}{2} \norm{\bm \Sigma^{-1/2}(\bm y - \bm m)}_2^2$;
\State $q \gets \frac{1}{4 \delta}\norm{\bm x - \bm \xi}_2^2$;
\State $\tilde q \gets \frac{1}{4 \delta}\norm{\bm y - \bm \zeta}_2^2$;
\State $s \gets - \tilde h + h - \tilde q + q$;
\State $r \gets \min(1, e^s)$;
\State Sample $\eta \sim \mathrm{U}([0,1])$;
\If{$r \geq \eta$}
\State $\bm x_{n+1} \gets \bm y$;
\Else
\State $\bm x_{n+1} \gets \bm x$;
\EndIf
\EndIf
\EndFor
\State return $\bm x_1, \ldots, \bm x_N$;
\end{algorithmic}
\end{algorithm}


\subsubsection*{MYMALA}

For MYMALA, the proposal distribution is \eqref{eq:mymala_proposal} the Hastings ratio can be computed by
\begin{align*}
s_n & = \log p(\*y) - \log p(\*x) + \log q_n(\*x_n | \*y) - \log q_n(\*y | \*x_n) \\
& = -h(\*y) + h(\*x_n) - \frac{1}{4 \delta_{n+1}} \norm{x_n - \hat y_n}_2^2 + \frac{1}{4 \delta_{n+1}} \norm{y - \hat x_n}_2^2, \\ \text{where} \quad \hat x_n & = x_n - \delta_{n+1} \left[\Sigma^{-1}(x_n - m) + \lambda^{-1}(x_n - \prox{g}{\lambda}(x_n)) \right]\\
\text{and} \quad \hat y_n & = y_n - \delta_{n+1} \left[\Sigma^{-1}(y_n - m) + \lambda^{-1}(y_n - \prox{g}{\lambda}(y_n)) \right].
\end{align*}

\begin{algorithm}[H]
\caption{\texttt{MYMALA}}
Given a feasible point $\bm x_0 \in \R^d$, sample size $N \in \N$, $\lambda > 0$, and a nonincreasing sequence $(\delta_n)_{n=1}^N > 0$;
\begin{algorithmic}[2]\label{alg:mymala}
\For{$n=0, 1,2,\ldots, N$}
\State $\bm x \gets \bm x_n$;
\State $\*u \gets \prox{g}{\lambda}(\*x)$;
\State $\*v \gets \* \Sigma^{-1}(\*x - \*m)$;
\State $\hat{\*x} \gets \bm x - \delta_{n+1} (\*v - \lambda^{-1}(\bm x - \bm u))$;
\State $\bm z \gets \normal(\bm 0, \Idmat_d)$;
\State $\bm y \gets \hat{\*x} + \sqrt{2 \delta_{n+1}} \bm z$;
\If{$\*y$ violates a constraint}
\State $\*x_{n+1} \gets \* x$
\Else
\State $\hat{\* u} \gets \prox{g}{\lambda}(\*y)$
\State $\hat{\* v} \gets \* \Sigma^{-1}(\*y - \*m)$;
\State $\hat{\* y} \gets \*y - \delta_{n+1} (\hat{\*v} - \lambda^{-1}(\*y - \hat{\* u}))$;
\State $h \gets \frac{1}{2}\norm{\Sigma^{-1/2}(x-m)}_2^2$;
\State $\tilde h \gets \frac{1}{2} \norm{\Sigma^{-1/2}(y-m)}_2^2$;
\State $q \gets \frac{1}{4 \delta_{n+1}} \norm{\*x - \hat y}_2^2$;
\State $\tilde q \gets \frac{1}{4 \delta_{n+1}} \norm{\*y - \hat x}_2^2$
\State $s \gets - \tilde h + h - \tilde q + q$;
\State $r \gets \min(1, e^s)$;
\State Sample $\eta \sim U([0,1])$;
\If{$r \geq \eta$}
\State $\*x_{n+1} \gets \*y$;
\Else
\State $\*x_{n+1} \gets \*x$;
\EndIf
\EndIf
\EndFor
\State return $\bm x_1, \ldots, \bm x_N$;
\end{algorithmic}
\end{algorithm}



\section{Indicators of convergence}

Assessing the quality of an MCMC run is notoriously difficult. We present a couple of numerical quantities that allow the user to judge whether the obtained samples represent the probability distribution of interest.

In the following, we assume that we are given $M$ independently run MCMC chains $(\*x_n^{(1)})_{n=1}^N, \ldots, (\*x_n^{(M)})_{n=1}^N$, each of length $N$. The subsequent computations also make sense for a single chain ($M=1$), but it is in general recommended to perform multiple MCMC runs if at all possible.

Let us fix some notation that we will make heavy use of in the rest of this section: For $m=1,\ldots, M$, let
\begin{align*}
\bar{\*x}_N^{m} = \frac{1}{N} \sum_{n=1}^N \*x_n^{(m)}
\end{align*}
be the sample average and
\begin{align*}
\*\Sigma_m = \frac{1}{N-1} \sum_{n=1}^N (x_n^{(m)} - \bar x^{(m)}) (x_n^{(m)} - \bar x^{(m)})^\top
\end{align*}
be the sample covariance of the $m$-th chain. Furthermore, let
\begin{align*}
\bar{\*x}_N = \frac{1}{M} \sum_{m=1}^M \bar{\*x}_N^{m}
\end{align*}
be the sample average and
\begin{align*}
\*\Sigma = \frac{1}{M} \sum_{m=1}^M \* \Sigma_m
\end{align*}
be the sample covariance of all chains.



\subsection{Effective sample size}

A very intuitive tool for a-posteriori assessments of the quality of a given MCMC chain is the so-called \emph{effective sample size}. Its multivariate version is described in \cite{VatFleJon19}. Intuitively, the effective sample size gives the number of independent samples with the same estimation power as the MCMC samples. That is, estimating a quantity of interest using an MCMC sample with effective sample size $N_\text{eff}$ is comparable to an estimate that uses $N_\text{eff}$ independent samples from the posterior distribution.

\subsubsection*{Definition of the effective sample size estimator}

The effective sample size can be estimated by
\begin{align*}
\widehat {\mathrm{ESS}} = MN \left(\frac{\det(\*\Sigma)}{\det(\*T_L)} \right)^{1/d},
\end{align*}
where
\begin{itemize}
\item $\* \Sigma$ is the sample covariance matrix of all chains as described above;
\item $\hat{\* T}_L$ is the so-called \emph{multivariate replicated lugsail batch means estimator} for the Monte Carlo standard error. Given a batch size $b < N$, let $a = \lfloor b \cdot N \rfloor$ denote the number of batches. Typically, the batch size must increase with $N$, and default choices are $b = \lfloor N^{1/3} \rfloor$ or $b = \lfloor N^{1/2} \rfloor$. For $i = 1,\ldots, a$, let
\begin{align*}
\hat{\bm x}_i^{(m)} = \frac{1}{b} \sum_{j=1}^b \bm x_{(i-1)b + j}^{(m)}
\end{align*}
denote the $i$-th batch mean for the $m$-th chain. Let
\begin{align*}
\hat{\*T}_b = \frac{b}{aM - 1} \sum_{m=1}^M \sum_{i=1}^a (\bar{\* x}_i^{(m)} - \bar{\*x})(\bar{\* x}_i^{(m)} - \bar{\*x})^\top.
\end{align*}
Then, $\hat{\*T}_L$ is defined as
\begin{align*}
\hat T_L = 2 \hat T_b - \hat T_{b/3}.
\end{align*}
\end{itemize}

\subsubsection*{Usage}

The authors of \cite{VatFleJon19} propose to stop the sampling once
\begin{align*}
\widehat{\mathrm{ESS}} \geq W(d, \alpha, \epsilon),
\end{align*}
with the number $W(d, \alpha, \epsilon)$ defined as 
\begin{align}
W(d, \alpha, \epsilon) = \frac{2^{2/d} \pi \chi^2_{1-\alpha,d}}{(d \Gamma(d/2))^{2/d} \epsilon^2}, \label{eq:w_d_alpha_epsilon}
\end{align}
where
\begin{itemize}
\item $d$ is the dimension of the parameter space.
\item $\alpha$ is the desired confidence level (e.g. $\alpha=0.05$ for 95\%-confidence.
\item $\epsilon$ is the desired relative precision. That is, a value of $\epsilon=0.1$ means that approximately 10\% of the variability in the samples come from the Monte Carlo error.
\item $\chi^2_{1-\alpha, d}$ is the $(1-\alpha)$-percentile of the chi-squared distribution with $d$ degrees of freedom.
\item $\Gamma$ is the gamma function, $\Gamma(z) = \int_0^\infty x^{z-1} e^{-x} \d x$.
\end{itemize}

\subsection{R hat}

The $\hat R$ statistic (also known as Gelman-Rubin statistic or  potential scale reduction factor (PSRF)) is the most widely used convergence diagnostic for MCMC. The current state-of-the-art for computing $\hat R$ in the univariate case is described in \cite{VehGelSimCarBur21}.

We implement the stabilized, multivariate version of $\hat R$ described in \cite{VatKnu21}. The precise definition is provided next.

\subsubsection*{Definition of R hat}

In \cite{VatKnu21}, the multivariate stabilized $\hat R$ statistic is defined as
\begin{align*}
\hat R^d_L = \sqrt{ \frac{N-1}{N} + \frac{\det(\*\Sigma^{-1} \hat{\* T}_L)^{1/d}}{N} },
\end{align*}
where $\*\Sigma$ and $\hat{\*T}_L$ are as above.


\subsubsection*{Usage}

A simple rule-of-thumb \cite{VehGelSimCarBur21} is that samples should only be used if $\hat R^d < 1.01$.

The authors of \cite{VatKnu21} provide an alternative criterion that is motivated by the relation of $\hat R^d$ to the effective sample size. They recommend that samples should only be used if
\begin{align*}
\hat R^d_L \leq \sqrt{1 + \frac{M}{W(\alpha, \epsilon, d)}},
\end{align*}
where $W(\alpha, \epsilon, d)$ is defined in \eqref{eq:w_d_alpha_epsilon}.






\addcontentsline{toc}{section}{Bibliography}

\printbibliography

\end{document}
